{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Wikipedia Articles\n",
    "In this analysis, I'll use `TruncatedSVD` to perform Principle Component Analysis on sparse arrays in `csr_matrix` format--in this case word-frequency arrays. First, I'll build the necessary machine learning pipeline using `TruncatedSVD` and k-means to cluster some popular pages from Wikipedia. Then, I'll apply it to the word-frequency array of some popular Wikipedia articles.\n",
    "\n",
    "The data are available from [Lateral.io](https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Wikipedia articles as a dataframe\n",
    "df = pd.read_csv('datasets/wikipedia-vectors.csv', index_col=0)\n",
    "\n",
    "# Transpose the dataframe otherwise there will be 13,000 columns (corresponding to the 13,000 words in the file)\n",
    "articles = csr_matrix(df.transpose())\n",
    "titles = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          article  label\n",
      "29                               Jennifer Aniston      0\n",
      "28                                  Anne Hathaway      0\n",
      "27                                 Dakota Fanning      0\n",
      "26                                     Mila Kunis      0\n",
      "25                                  Russell Crowe      0\n",
      "24                                   Jessica Biel      0\n",
      "23                           Catherine Zeta-Jones      0\n",
      "22                              Denzel Washington      0\n",
      "21                             Michael Fassbender      0\n",
      "20                                 Angelina Jolie      0\n",
      "19  2007 United Nations Climate Change Conference      1\n",
      "18  2010 United Nations Climate Change Conference      1\n",
      "16                                        350.org      1\n",
      "15                                 Kyoto Protocol      1\n",
      "17  Greenhouse gas emissions by the United States      1\n",
      "13                               Connie Hedegaard      1\n",
      "12                                   Nigel Lawson      1\n",
      "11       Nationally Appropriate Mitigation Action      1\n",
      "10                                 Global warming      1\n",
      "14                                 Climate change      1\n",
      "44                                           Gout      2\n",
      "46                                     Prednisone      2\n",
      "47                                          Fever      2\n",
      "48                                     Gabapentin      2\n",
      "42                                    Doxycycline      2\n",
      "49                                       Lymphoma      2\n",
      "43                                       Leukemia      2\n",
      "54                                 Arctic Monkeys      3\n",
      "55                                  Black Sabbath      3\n",
      "56                                       Skrillex      3\n",
      "57                          Red Hot Chili Peppers      3\n",
      "51                                     Nate Ruess      3\n",
      "50                                   Chad Kroeger      3\n",
      "52                                     The Wanted      3\n",
      "53                                   Stevie Nicks      3\n",
      "59                                    Adam Levine      3\n",
      "58                                         Sepsis      3\n",
      "0                                        HTTP 404      4\n",
      "1                                  Alexa Internet      4\n",
      "2                               Internet Explorer      4\n",
      "3                                     HTTP cookie      4\n",
      "40                                    Tonsillitis      4\n",
      "41                                    Hepatitis B      4\n",
      "4                                   Google Search      4\n",
      "5                                          Tumblr      4\n",
      "6                     Hypertext Transfer Protocol      4\n",
      "45                                    Hepatitis C      4\n",
      "7                                   Social search      4\n",
      "8                                         Firefox      4\n",
      "9                                        LinkedIn      4\n",
      "36              2014 FIFA World Cup qualification      5\n",
      "31                              Cristiano Ronaldo      5\n",
      "32                                   Arsenal F.C.      5\n",
      "33                                 Radamel Falcao      5\n",
      "34                             Zlatan Ibrahimović      5\n",
      "39                                  Franck Ribéry      5\n",
      "38                                         Neymar      5\n",
      "37                                       Football      5\n",
      "35                Colombia national football team      5\n",
      "30                  France national football team      5\n"
     ]
    }
   ],
   "source": [
    "# Create a TruncatedSVD instance\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# Create a KMeans instance\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = make_pipeline(svd, kmeans)\n",
    "\n",
    "# Fit the pipeline to articles\n",
    "pipeline.fit(articles)\n",
    "\n",
    "# Calculate the cluster labels\n",
    "labels = pipeline.predict(articles)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discovering interpretable features\n",
    "As you can see, the above articles cluster into well determinable clusters. We have a topic that looks like 'internet technologies', and another for 'football', and another for 'actors', etc.\n",
    "\n",
    "In the following, I'll employ a dimension reduction technique called \"Non-negative matrix factorization\" (NMF) that expresses samples as combinations of interpretable parts. For example, it expresses documents as combinations of topics. To apply NMF, I'll use the tf-idf word-frequency array of Wikipedia articles that I created earlier, given as a csr matrix. I'll fit the model and transform the articles, and then I'll explore the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.44052876]\n",
      " [0.         0.         0.         0.         0.         0.56668499]\n",
      " [0.00382019 0.         0.         0.         0.         0.39870403]\n",
      " [0.         0.         0.         0.         0.         0.381795  ]\n",
      " [0.         0.         0.         0.         0.         0.48558457]\n",
      " [0.01292803 0.01378894 0.00776331 0.03344715 0.         0.33456931]\n",
      " [0.         0.         0.02067421 0.         0.00604557 0.35911324]\n",
      " [0.         0.         0.         0.         0.         0.49104488]\n",
      " [0.01542612 0.01428165 0.00376631 0.02371005 0.02626534 0.48084359]\n",
      " [0.01117295 0.03136769 0.03094882 0.06569705 0.01967034 0.33833672]]\n"
     ]
    }
   ],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance\n",
    "model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles\n",
    "model.fit(articles)\n",
    "\n",
    "# Transform the articles\n",
    "nmf_features = model.transform(articles)\n",
    "\n",
    "# Print the first 10 NMF features\n",
    "print(nmf_features[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.003845\n",
      "1    0.000000\n",
      "2    0.000000\n",
      "3    0.575686\n",
      "4    0.000000\n",
      "5    0.000000\n",
      "Name: Anne Hathaway, dtype: float64\n",
      "0    0.000000\n",
      "1    0.005601\n",
      "2    0.000000\n",
      "3    0.422361\n",
      "4    0.000000\n",
      "5    0.000000\n",
      "Name: Denzel Washington, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(nmf_features, index=titles)\n",
    "\n",
    "# Print the row for 'Anne Hathaway'\n",
    "print(df.loc['Anne Hathaway'])\n",
    "\n",
    "# Print the row for 'Denzel Washington'\n",
    "print(df.loc['Denzel Washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF features of the Wikipedia articles\n",
    "When investigating the features above, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. This is because NMF components represent topics (for instance, acting). When NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. \n",
    "\n",
    "Verifying this with the NMF model I have can be done. We saw above that the 3rd NMF feature value was high for articles about both actors Denzel Washington and Anne Hathaway. Now, I'll demonstrate how to identify the topic of the corresponding NMF component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 13125)\n",
      "film       0.627904\n",
      "award      0.253143\n",
      "starred    0.245295\n",
      "role       0.211460\n",
      "actress    0.186406\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import the vocabulary csv and transform it into a list of words.\n",
    "words_df = pd.read_csv('datasets/wikipedia-vocabulary.txt', header=None, names=['words'])\n",
    "words = words_df.words.tolist()\n",
    "\n",
    "# Create a DataFrame\n",
    "components_df = pd.DataFrame(model.components_, columns=words)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "\n",
    "# Select row 3 corresponding with the 3rd feature\n",
    "component = components_df.iloc[3]\n",
    "\n",
    "# call and print nlarges on component, this gives the five words with the highest values for that component.\n",
    "print(component.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF features are topics\n",
    "It's now easy to recognise the topics that the articles about Anne Hathaway and Denzel Washington have in common! It certainly seems to make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo                1.000000\n",
      "Franck Ribéry                    0.999972\n",
      "Radamel Falcao                   0.999942\n",
      "Zlatan Ibrahimović               0.999942\n",
      "France national football team    0.999923\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the NMF features\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(norm_features, index=titles)\n",
    "\n",
    "# Select the row corresponding to 'Cristiano Ronaldo'\n",
    "article = df.loc[\"Cristiano Ronaldo\"]\n",
    "\n",
    "# Compute the dot products\n",
    "similarities = df.dot(article)\n",
    "\n",
    "# Display those with the largest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion Finding Similar Articles\n",
    "Finally, I've quickly demonstrated how to use NMF features and the cosine similarity to find similar articles. I applied this to the NMF model for popular Wikipedia articles, by finding the articles most similar to the article about the footballer Cristiano Ronaldo. As you can see, we get back mostly other footballers and an article about a football team. Pretty cool! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
