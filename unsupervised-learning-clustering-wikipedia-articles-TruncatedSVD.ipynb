{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Wikipedia Articles\n",
    "In this analysis, I'll use `TruncatedSVD` to perform Principle Component Analysis on sparse arrays in `csr_matrix` format--in this case word-frequency arrays. First, I'll build the necessary machine learning pipeline using `TruncatedSVD` and k-means to cluster some popular pages from Wikipedia. Then, I'll apply it to the word-frequency array of some popular Wikipedia articles.\n",
    "\n",
    "The data are available from [Lateral.io](https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Wikipedia articles as a dataframe\n",
    "df = pd.read_csv('datasets/wikipedia-vectors.csv', index_col=0)\n",
    "\n",
    "# Transpose the dataframe otherwise there will be 13,000 columns (corresponding to the 13,000 words in the file)\n",
    "articles = csr_matrix(df.transpose())\n",
    "titles = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          article  label\n",
      "0                                        HTTP 404      0\n",
      "8                                         Firefox      0\n",
      "7                                   Social search      0\n",
      "6                     Hypertext Transfer Protocol      0\n",
      "5                                          Tumblr      0\n",
      "9                                        LinkedIn      0\n",
      "3                                     HTTP cookie      0\n",
      "2                               Internet Explorer      0\n",
      "1                                  Alexa Internet      0\n",
      "4                                   Google Search      0\n",
      "21                             Michael Fassbender      1\n",
      "28                                  Anne Hathaway      1\n",
      "27                                 Dakota Fanning      1\n",
      "26                                     Mila Kunis      1\n",
      "25                                  Russell Crowe      1\n",
      "24                                   Jessica Biel      1\n",
      "23                           Catherine Zeta-Jones      1\n",
      "22                              Denzel Washington      1\n",
      "20                                 Angelina Jolie      1\n",
      "29                               Jennifer Aniston      1\n",
      "39                                  Franck Ribéry      2\n",
      "38                                         Neymar      2\n",
      "37                                       Football      2\n",
      "36              2014 FIFA World Cup qualification      2\n",
      "35                Colombia national football team      2\n",
      "34                             Zlatan Ibrahimović      2\n",
      "33                                 Radamel Falcao      2\n",
      "32                                   Arsenal F.C.      2\n",
      "31                              Cristiano Ronaldo      2\n",
      "30                  France national football team      2\n",
      "19  2007 United Nations Climate Change Conference      3\n",
      "18  2010 United Nations Climate Change Conference      3\n",
      "10                                 Global warming      3\n",
      "11       Nationally Appropriate Mitigation Action      3\n",
      "12                                   Nigel Lawson      3\n",
      "13                               Connie Hedegaard      3\n",
      "14                                 Climate change      3\n",
      "15                                 Kyoto Protocol      3\n",
      "16                                        350.org      3\n",
      "17  Greenhouse gas emissions by the United States      3\n",
      "57                          Red Hot Chili Peppers      4\n",
      "56                                       Skrillex      4\n",
      "55                                  Black Sabbath      4\n",
      "54                                 Arctic Monkeys      4\n",
      "53                                   Stevie Nicks      4\n",
      "52                                     The Wanted      4\n",
      "51                                     Nate Ruess      4\n",
      "50                                   Chad Kroeger      4\n",
      "59                                    Adam Levine      4\n",
      "58                                         Sepsis      4\n",
      "49                                       Lymphoma      5\n",
      "47                                          Fever      5\n",
      "46                                     Prednisone      5\n",
      "44                                           Gout      5\n",
      "43                                       Leukemia      5\n",
      "42                                    Doxycycline      5\n",
      "41                                    Hepatitis B      5\n",
      "40                                    Tonsillitis      5\n",
      "48                                     Gabapentin      5\n",
      "45                                    Hepatitis C      5\n"
     ]
    }
   ],
   "source": [
    "# Create a TruncatedSVD instance\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# Create a KMeans instance\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = make_pipeline(svd, kmeans)\n",
    "\n",
    "# Fit the pipeline to articles\n",
    "pipeline.fit(articles)\n",
    "\n",
    "# Calculate the cluster labels\n",
    "labels = pipeline.predict(articles)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('label'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discovering interpretable features\n",
    "As you can see above articles cluster into well determinable clusters. We have a topic that looks like 'internet technologies', and another for 'football', and another for 'actors', etc.\n",
    "\n",
    "In the following, I'll employ a dimension reduction technique called \"Non-negative matrix factorization\" (NMF) that expresses samples as combinations of interpretable parts. For example, it expresses documents as combinations of topics. To apply NMF, I'll use the tf-idf word-frequency array of Wikipedia articles that I created earlier, given as a csr matrix. I'll fit the model and transform the articles, and then I'll explore the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.44044633]\n",
      " [0.         0.         0.         0.         0.         0.56658049]\n",
      " [0.00382077 0.         0.         0.         0.         0.39862949]\n",
      " [0.         0.         0.         0.         0.         0.38172339]\n",
      " [0.         0.         0.         0.         0.         0.48549606]\n",
      " [0.0129298  0.01378914 0.00776279 0.03344405 0.         0.33450788]\n",
      " [0.         0.         0.02067311 0.         0.00604506 0.35904555]\n",
      " [0.         0.         0.         0.         0.         0.49095545]\n",
      " [0.01542826 0.01428189 0.00376613 0.023708   0.02626278 0.48075388]\n",
      " [0.01117449 0.03136815 0.03094688 0.06569109 0.01966827 0.33827458]]\n"
     ]
    }
   ],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance\n",
    "model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles\n",
    "model.fit(articles)\n",
    "\n",
    "# Transform the articles\n",
    "nmf_features = model.transform(articles)\n",
    "\n",
    "# Print the first 10 NMF features\n",
    "print(nmf_features[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.003846\n",
      "1    0.000000\n",
      "2    0.000000\n",
      "3    0.575634\n",
      "4    0.000000\n",
      "5    0.000000\n",
      "Name: Anne Hathaway, dtype: float64\n",
      "0    0.000000\n",
      "1    0.005601\n",
      "2    0.000000\n",
      "3    0.422324\n",
      "4    0.000000\n",
      "5    0.000000\n",
      "Name: Denzel Washington, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(nmf_features, index=titles)\n",
    "\n",
    "# Print the row for 'Anne Hathaway'\n",
    "print(df.loc['Anne Hathaway'])\n",
    "\n",
    "# Print the row for 'Denzel Washington'\n",
    "print(df.loc['Denzel Washington'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF features of the Wikipedia articles\n",
    "When investigating the features above, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. This is because NMF components represent topics (for instance, acting). When NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. \n",
    "\n",
    "Verifying this with the NMF model I have can be done. We saw above that the 3rd NMF feature value was high for articles about both actors Denzel Washington and Anne Hathaway. Now, I'll demonstrate how to identify the topic of the corresponding NMF component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 13125)\n",
      "filming    0.627960\n",
      "award      0.253165\n",
      "stated     0.245317\n",
      "romance    0.211479\n",
      "actress    0.186422\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import the words csv and transform it into a list of words.\n",
    "words_df = pd.read_csv('datasets/wikipedia-words.csv', names=['words'], index_col=False)\n",
    "words = words_df.words.tolist()\n",
    "\n",
    "# Create a DataFrame\n",
    "components_df = pd.DataFrame(model.components_, columns=words)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "\n",
    "# Select row 3 corresponding with the 3rd feature\n",
    "component = components_df.iloc[3]\n",
    "\n",
    "# call and print nlarges on component, this gives the five words with the highest values for that component.\n",
    "print(component.nlargest())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF features are topics\n",
    "It's now easy to recognise the topics that the articles about Anne Hathaway and Denzel Washington have in common!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo                1.000000\n",
      "Franck Ribéry                    0.999972\n",
      "Radamel Falcao                   0.999942\n",
      "Zlatan Ibrahimović               0.999942\n",
      "France national football team    0.999923\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the NMF features\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(norm_features, index=titles)\n",
    "\n",
    "# Select the row corresponding to 'Cristiano Ronaldo'\n",
    "article = df.loc[\"Cristiano Ronaldo\"]\n",
    "\n",
    "# Compute the dot products\n",
    "similarities = df.dot(article)\n",
    "\n",
    "# Display those with the largest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion Finding Similar Articles\n",
    "Finally, I've quickly demonstrated how to use NMF features and the cosine similarity to find similar articles. I applied this to the NMF model for popular Wikipedia articles, by finding the articles most similar to the article about the footballer Cristiano Ronaldo. As you can see, we get back mostly other footballers and an article about a football team. Pretty cool! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
